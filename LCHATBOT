#cell 1:

import os
from pathlib import Path
from dotenv import load_dotenv
import pytesseract
from pdf2image import convert_from_path
from pydantic import BaseModel, Field
from typing import List, Dict

# Load environment variables if needed
load_dotenv()

# Define the Section and StructuredDocument classes
class Section(BaseModel):
    title: str = Field(description="Concise section title (e.g., 'Rent', 'Term', 'Security Deposit')")
    start_index: int = Field(description="Starting line number")
    end_index: int = Field(description="Ending line number")
    summary: str = Field(description="Concise summary containing key facts, figures, and specifics")
    key_points: List[str] = Field(
        description="Bullet points of critical information (dates, amounts, requirements)",
        min_items=1
    )

class StructuredDocument(BaseModel):
    sections: List[Section] = Field(description="Key lease sections with summaries")
    property_address: str = Field(description="Full property address")
    parties: dict = Field(description="Key parties involved (lessor, lessee, etc.)")

def extract_text_from_pdf(pdf_path: str) -> str:
    # Convert PDF pages to images and extract text using Tesseract
    images = convert_from_path('../Lease/leasedocs/tolucalease.pdf')
    extracted_text = ""

    for image in images:
        extracted_text += pytesseract.image_to_string(image)
    
    return extracted_text

def doc_with_lines(document: str) -> Dict[int, str]:
    document_lines = document.split("\n")
    line2text = {}
    
    for i, line in enumerate(document_lines):
        line2text[i] = line.strip()  # Store stripped lines
    
    return line2text

async def process_faq_document(pdf_path: str) -> None:
    # Extract text
    extracted_text = extract_text_from_pdf(pdf_path)
    
    # Assign line numbers to extracted text
    indexed_lines = doc_with_lines(extracted_text)
    
    # Print indexed lines for debugging
    for index, line in indexed_lines.items():
        print(f"[{index}] {line}")
    
    # Here, you can implement logic to detect sections in your FAQ document
    # and create Section instances for each section found in the extracted text
    # For now, we are just printing the lines, but this is where the parsing would occur.

# Test the process on your FAQ document
async def main():
    faq_pdf_path = '../FAQ/Your_FAQ_Document.pdf'  # Path to your FAQ PDF


    await process_faq_document(faq_pdf_path)

await main()

#cell 2:

import os
from dotenv import load_dotenv

load_dotenv()
print("Tesseract Path from .env:", os.getenv('TESSERACT_PATH'))


#cell 3:

import sys
import os
from dotenv import load_dotenv
import instructor
import openai
from typing import List, Dict
from pydantic import BaseModel, Field
from tenacity import Retrying, stop_after_attempt, wait_fixed
from IPython.display import Markdown, display
import re

# Append the src directory to the system path to allow imports
sys.path.append(os.path.abspath(os.path.join(os.getcwd(), 'src')))
load_dotenv()

# Import the constants module for OpenAI model reference
import constants as consts

# Initialize the OpenAI client with Azure endpoint settings
client = instructor.patch(
    openai.AzureOpenAI(
        azure_endpoint=os.getenv("CORTEXT_API_BASE"),
        api_version=os.getenv("CORTEX_API_VERSION"),
        api_key=os.getenv("AZURE_OPENAI_API_KEY"),
        default_headers={"use-case": "FAQ Extraction"},
    ),
)

# Updated system prompt for FAQ extraction
system_prompt = f"""\ 
You are an expert information extraction AI. Your goal is to identify and structure key concepts from the FAQ document. Follow these guidelines:

1. ORGANIZATION:
- Identify and create sections based on significant topics discussed.
- Focus on self-contained sections that capture the essence of each topic.
- Maintain consistent formatting for clarity and easy reading.
- Use clear section headers in ALL CAPS.

2. CONTENT REQUIREMENTS:
For each section:
- Capture the main question and its corresponding answer.
- Provide concise summaries and relevant details.
- Include bullet points for clarity, if applicable.

3. ESSENTIAL SECTIONS TO IDENTIFY:
- General Information: Overview of the topic.
- Specific Queries: Detailed answers to common questions.
- Important Policies: Key guidelines or regulations related to the FAQ.
- Contact Information: Who to reach out to for more questions.

4. ADDITIONAL GUIDELINES:
- Highlight unique or important points that may require attention.
- Ensure the format is easy to parse for retrieval in a Q&A system.
- Use clear, direct language that is easy to understand.

5. FORMAT REQUIREMENTS:
- Use ALL CAPS for section headers.
- Use bullet points for easy scanning where applicable.
- Maintain a clear separation between distinct topics.

Remember:
- Be thorough but concise - each section should deliver value.
- Focus on clarity and self-containment in the content you provide.
"""

# Define the Section data model
class Section(BaseModel):
    title: str = Field(description="Title of the section.")
    content: str = Field(description="Content of the section.")
    start_index: int = Field(description="Starting line number")
    end_index: int = Field(description="Ending line number")
    key_points: List[str] = Field(default_factory=list)  # Initialize as empty list if not provided

def get_structured_document(document_with_line_numbers: str) -> List[Section]:
    response = client.chat.completions.create(
        model=consts.OpenAIModel.GPT_4_OMNI,
        messages=[
            {
                "role": "system",
                "content": system_prompt,
            },
            {
                "role": "user",
                "content": document_with_line_numbers,
            },
        ],
    )
    
    # Process the response to extract structured sections
    structured_sections = response.choices[0].message.content.strip()
    
    # Debug: Print the structured sections to understand its format
    print("Structured Sections Output:\n", structured_sections)

    sections = []
    start_index = 0  # Initialize the start index
    
    # Use regex to find section headers and contents
    section_pattern = re.compile(r'(.*?)(?:\n\n|\Z)', re.DOTALL)  # Match any content until double newlines or end of string
    matches = section_pattern.findall(structured_sections)
    
    for match in matches:
        if match.strip():  # Only process non-empty sections
            # This assumes the first line is the title
            parts = match.strip().split('\n', 1)  # Split on the first newline
            if len(parts) == 2:  # Ensure we have both title and content
                title, content = parts
                end_index = start_index + content.count('\n') + 1  # Calculate end_index based on line count
                sections.append(Section(title=title.strip(), content=content.strip(), start_index=start_index, end_index=end_index))
                start_index = end_index  # Update start_index for the next section
            else:
                print(f"Skipping malformed section: {match}")
    
    return sections

def extract_text_from_pdf(pdf_path: str) -> str:
    from pdf2image import convert_from_path
    import pytesseract

    # Convert PDF pages to images and extract text using Tesseract
    images = convert_from_path(pdf_path)
    extracted_text = ""

    for image in images:
        extracted_text += pytesseract.image_to_string(image)
    
    return extracted_text

def doc_with_lines(document: str) -> Dict[int, str]:
    document_lines = document.split("\n")
    line2text = {}
    
    for i, line in enumerate(document_lines):
        line2text[i] = line.strip()  # Store stripped lines
    
    return line2text

def get_sections_text(structured_doc: List[Section], line2text: Dict[int, str]) -> List[Dict[str, str]]:
    segments = []
    for s in structured_doc:
        contents = []
        for line_id in range(s.start_index, s.end_index):
            contents.append(line2text.get(line_id, ''))
        content_text = "\n".join(contents)
        segments.append({
            "title": s.title,
            "content": content_text,
            "summary": content_text,  # Using content as summary for backward compatibility
            "start": s.start_index,
            "end": s.end_index
        })
    return segments

def create_markdown_summary(structured_doc: List[Section]) -> str:
    markdown_string = "# FAQ Summary\n\n"

    # Key Sections
    for section in structured_doc:
        markdown_string += f"## {section.title}\n"
        markdown_string += f"{section.content}\n\n"
        
        markdown_string += "**Key Points:**\n"
        for point in section.key_points:
            markdown_string += f"- {point}\n"
        
        markdown_string += "\n---\n\n"
    
    return markdown_string

async def process_faq_document(pdf_path: str) -> None:
    # Extract text from the PDF
    extracted_text = extract_text_from_pdf(pdf_path)
    
    # Assign line numbers to the extracted text
    indexed_lines = doc_with_lines(extracted_text)
    
    # Convert the indexed lines to a single string format for the AI model
    document_with_line_numbers = "\n".join(f"[{i}] {line}" for i, line in indexed_lines.items())
    
    # Call the method to get structured documents
    structured_doc = get_structured_document(document_with_line_numbers)  # Get structured sections
    
    # Convert sections to text segments
    segments = get_sections_text(structured_doc, indexed_lines)
    
    # Display segments count
    print(f"Total Segments: {len(segments)}")
    
    # Display the first segment for validation
    print("First Segment:\n", segments[0])
    
    # Generate and display the markdown summary
    markdown_display = create_markdown_summary(structured_doc)
    display(Markdown(markdown_display))

# Test the process on your FAQ document
async def main():
    faq_pdf_path = '../Lease/leasedocs/tolucalease.pdf'  # Path to your FAQ PDF
    await process_faq_document(faq_pdf_path)

await main()



#New Version:

# Install required packages
!pip install pypdf langdetect deep-translator pandas
!pip install pkg_resources

import os
from pypdf import PdfReader

def extract_text_from_pdf(pdf_path):
    """Extract text from a PDF file."""
    reader = PdfReader(pdf_path)
    text = ""
    for page in reader.pages:
        text += page.extract_text() + "\n"
    return text

from langdetect import detect
from deep_translator import GoogleTranslator

def detect_language(text):
    """Detect the language of the input text."""
    try:
        return detect(text[:1000])  # Use first 1000 chars for detection
    except:
        return "en"  # Default to English if detection fails

def translate_text(text, source_lang="auto", target_lang="en"):
    """Translate text to the target language."""
    try:
        # Handle long texts by breaking them into chunks
        max_chunk_size = 5000  # GoogleTranslator limit
        result = ""
        
        # Process text in chunks
        for i in range(0, len(text), max_chunk_size):
            chunk = text[i:i + max_chunk_size]
            translated_chunk = GoogleTranslator(source=source_lang, target=target_lang).translate(chunk)
            result += translated_chunk + " "
            
        return result.strip()
    except Exception as e:
        print(f"Translation error: {e}")
        return text  # Return original text if translation fails


import pandas as pd
import uuid

def process_document(pdf_path, output_dir):
    """Process PDF: extract, detect language, translate, and save."""
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    
    # Extract text
    original_text = extract_text_from_pdf(pdf_path)
    
    # Get filename without extension
    base_filename = os.path.splitext(os.path.basename(pdf_path))[0]
    
    # Detect language
    detected_lang = detect_language(original_text)
    print(f"Detected language: {detected_lang}")
    
    # Save original text
    original_path = os.path.join(output_dir, f"{base_filename}_original_{detected_lang}.txt")
    with open(original_path, 'w', encoding='utf-8') as f:
        f.write(original_text)
    
    # Translate if not already English
    if detected_lang != 'en':
        translated_text = translate_text(original_text, source_lang=detected_lang, target_lang='en')
        translated_path = os.path.join(output_dir, f"{base_filename}_translated_en.txt")
        with open(translated_path, 'w', encoding='utf-8') as f:
            f.write(translated_text)
    else:
        translated_text = original_text
        translated_path = original_path
    
    return {
        'original_path': original_path,
        'translated_path': translated_path,
        'language': detected_lang,
        'original_text': original_text,
        'translated_text': translated_text
    }



def create_indexed_sections(text, section_patterns=None):
    """
    Index lines of text and identify sections.
    
    Args:
        text: The text to process
        section_patterns: List of regex patterns to identify section headers
    
    Returns:
        DataFrame with indexed lines and section information
    """
    import re
    
    # Default section patterns if none provided
    if section_patterns is None:
        section_patterns = [
            r'^#+\s+(.+)$',  # Markdown headers
            r'^(\d+\.\s+.+)$',  # Numbered sections
            r'^([A-Z][A-Z\s]+)$',  # ALL CAPS sections
            r'^(Article \d+\:)',  # Legal document sections
            r'^(Section \d+\:)'  # Legal document sections
        ]
    
    # Split text into lines and create initial dataframe
    lines = text.split('\n')
    df = pd.DataFrame({
        'line_idx': range(len(lines)),
        'content': lines
    })
    
    # Remove empty lines
    df = df[df['content'].str.strip() != '']
    df = df.reset_index(drop=True)
    
    # Identify sections
    current_section = "INTRODUCTION"
    sections = []
    section_levels = []
    
    for idx, line in df.iterrows():
        is_section_header = False
        
        for pattern in section_patterns:
            if re.match(pattern, line['content'].strip()):
                current_section = line['content'].strip()
                is_section_header = True
                # Determine section level (basic implementation)
                if line['content'].startswith('#'):
                    level = len(re.match(r'^(#+)', line['content']).group(1))
                else:
                    level = 1
                break
        
        if not is_section_header:
            level = None
            
        sections.append(current_section)
        section_levels.append(level)
    
    df['section'] = sections
    df['section_level'] = section_levels
    df['is_section_header'] = [level is not None for level in section_levels]
    
    # Generate unique IDs for each entry
    df['id'] = [str(uuid.uuid4()) for _ in range(len(df))]
    
    return df



def process_multilingual_document(pdf_path, output_dir, section_patterns=None):
    """
    Complete pipeline to:
    1. Extract text from PDF
    2. Detect language
    3. Translate if needed
    4. Index and identify sections
    5. Save processed data
    """
    # Process the document
    doc_info = process_document(pdf_path, output_dir)
    
    # Create indexed sections for original text
    original_df = create_indexed_sections(
        doc_info['original_text'],
        section_patterns=section_patterns
    )
    original_df['language'] = doc_info['language']
    original_df['is_translated'] = False
    
    # Create indexed sections for translated text (if different from original)
    if doc_info['language'] != 'en':
        translated_df = create_indexed_sections(
            doc_info['translated_text'],
            section_patterns=section_patterns
        )
        translated_df['language'] = 'en'
        translated_df['is_translated'] = True
        
        # Save both dataframes
        original_df.to_excel(os.path.join(output_dir, f"{os.path.basename(pdf_path)}_original_indexed.xlsx"), index=False)
        translated_df.to_excel(os.path.join(output_dir, f"{os.path.basename(pdf_path)}_translated_indexed.xlsx"), index=False)
        
        return original_df, translated_df
    else:
        # Save just the original dataframe if already in English
        original_df.to_excel(os.path.join(output_dir, f"{os.path.basename(pdf_path)}_indexed.xlsx"), index=False)
        return original_df, None



# Example usage
pdf_path = "/path/to/your/document.pdf"
output_dir = "/path/to/output"

# Optional: Custom section patterns
custom_patterns = [
    r'^#+\s+(.+)$',             # Markdown headers
    r'^(\d+\.\d+\.\s+.+)$',     # Nested numbered sections
    r'^(APPENDIX [A-Z])'        # Appendices
]

# Process the document
original_df, translated_df = process_multilingual_document(
    pdf_path,
    output_dir,
    section_patterns=custom_patterns
)

print(f"Original document processed with {len(original_df)} lines")
if translated_df is not None:
    print(f"Translated document processed with {len(translated_df)} lines")



from llama_index import Document, ServiceContext, VectorStoreIndex
from llama_index.text_splitter import TokenTextSplitter
from llama_index.llms.azure_openai import AzureOpenAI
from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding

def create_llama_index(df, llm_client, embedding_client):
    """
    Create LlamaIndex from processed document dataframe
    """
    # Create documents from sections
    documents = []
    for section_name, group in df.groupby('section'):
        if any(group['is_section_header']):
            # Skip the section header itself
            group = group[~group['is_section_header']]
            
        # Join the content within this section
        content = "\n".join(group['content'].tolist())
        
        # Create a Document object
        doc = Document(
            text=content,
            metadata={
                "section": section_name,
                "language": group['language'].iloc[0],
                "is_translated": group['is_translated'].iloc[0],
                "line_ids": group['id'].tolist()
            }
        )
        documents.append(doc)
    
    # Create service context
    service_context = ServiceContext.from_defaults(
        llm=llm_client,
        embed_model=embedding_client
    )
    
    # Create vector store index
    index = VectorStoreIndex.from_documents(
        documents,
        service_context=service_context
    )
    
    return index


# Setup Azure clients (modified from your original code)
import os
os.environ["OPENAI_API_KEY"] = "redacted"

from llama_index.llms.azure_openai import AzureOpenAI
from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding

# Set up LLM client
azure_llm = AzureOpenAI(
    model="gpt-4o-2024-05-13",
    engine="gpt-4o-2024-05-13",
    api_key="redacted",
    azure_endpoint="https://cortex.aws.lmig.com/rest/v2/azure",
    default_headers={"use-case": "ERES Lease Test"}
)

# Set up embedding client
azure_embedding = AzureOpenAIEmbedding(
    model="text-embedding-ada-002",
    deployment="TEXT_EMBEDDING_ADA_002_V2",
    azure_endpoint="https://cortex.aws.lmig.com/rest/v2/azure",
    api_key=os.environ.get("CORTEX_API_TOKEN"),
    api_version="2023-09-01-preview"
)

# Process document
pdf_path = "/path/to/multilingual/document.pdf"
output_dir = "/path/to/output"
original_df, translated_df = process_multilingual_document(pdf_path, output_dir)

# Create indices
if translated_df is not None:
    # Create an index for the translated content
    translated_index = create_llama_index(translated_df, azure_llm, azure_embedding)
    translated_index.persist("/path/to/indices/translated_index")
    
    # Create an index for the original content
    original_index = create_llama_index(original_df, azure_llm, azure_embedding)
    original_index.persist("/path/to/indices/original_index")
else:
    # Just create one index if document was already in English
    index = create_llama_index(original_df, azure_llm, azure_embedding)
    index.persist("/path/to/indices/content_index")




