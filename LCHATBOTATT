# Cell 1: Environment Setup and Dependencies
import os
import re
import uuid
import pandas as pd
import time
import numpy as np
from PIL import Image
import pytesseract
from pdf2image import convert_from_path
from langdetect import detect
from deep_translator import GoogleTranslator
from langchain_openai.embeddings import AzureOpenAIEmbeddings
from langchain_openai import AzureOpenAI
from sklearn.metrics.pairwise import cosine_similarity
from typing import List, Dict, Any

# Set environment variables for API keys (replace with your actual keys in production)
def setup_environment_variables():
    """Set up necessary environment variables for API access"""
    os.environ["OPENAI_API_KEY"] = "redacted"
    os.environ["CORTEX_API_TOKEN"] = "redacted"
    os.environ["AZURE_OPENAI_API_KEY"] = "redacted"

# Run setup
setup_environment_variables()

# Cell 2: Document Processing Functions - Text Extraction and Language Detection
def extract_text_with_tesseract(pdf_path):
    """
    Extract text from a PDF file using Tesseract OCR.
    
    Args:
        pdf_path: Path to the PDF file
        
    Returns:
        Extracted text from all pages
    """
    try:
        # Convert PDF to images
        images = convert_from_path(pdf_path)
        
        # Extract text from each page image
        extracted_text = ""
        for i, img in enumerate(images):
            print(f"Processing page {i+1}/{len(images)}")
            text = pytesseract.image_to_string(img)
            extracted_text += text + "\n\n"
            
        return extracted_text
    except Exception as e:
        print(f"Error extracting text: {e}")
        return ""

def detect_language(text):
    """Detect the language of the input text."""
    try:
        # Use a substantial chunk of text for more accurate detection
        sample_text = text[:3000].replace('\n', ' ').strip()
        return detect(sample_text)
    except Exception as e:
        print(f"Language detection error: {e}")
        return "en"  # Default to English if detection fails

def translate_text(text, source_lang="auto", target_lang="en"):
    """Translate text to the target language."""
    try:
        if not text.strip():
            return ""
            
        # Handle long texts by breaking them into chunks
        max_chunk_size = 4000  # Safe limit for GoogleTranslator
        result = ""
        
        # Process text in chunks
        for i in range(0, len(text), max_chunk_size):
            chunk = text[i:i + max_chunk_size]
            if chunk.strip():  # Only translate non-empty chunks
                translated_chunk = GoogleTranslator(source=source_lang, target=target_lang).translate(chunk)
                result += translated_chunk + " "
                print(f"Translated chunk {i//max_chunk_size + 1}")
            
        return result.strip()
    except Exception as e:
        print(f"Translation error: {e}")
        return text  # Return original text if translation fails

# Cell 3: Document Section Indexing and Organization
def create_indexed_sections(text, section_patterns=None):
    """
    Index lines of text and identify sections.
    
    Args:
        text: The text to process
        section_patterns: List of regex patterns to identify section headers
    
    Returns:
        DataFrame with indexed lines and section information
    """
    # Default section patterns if none provided
    if section_patterns is None:
        section_patterns = [
            r'^#+\s+(.+)$',                # Markdown headers
            r'^(\d+\.(?:\d+\.)*\s+.+)$',   # Numbered sections (1., 1.1., etc)
            r'^([A-Z][A-Z\s]+:?)$',        # ALL CAPS sections
            r'^(Article\s+\d+[.:])(?:\s+|$)',  # Article sections
            r'^(Section\s+\d+[.:])(?:\s+|$)',  # Section headers
            r'^(CHAPTER\s+[IVX0-9]+)(?:\s+|$)' # Chapter headers (Roman or Arabic)
        ]
    
    # Split text into lines and create initial dataframe
    lines = text.split('\n')
    df = pd.DataFrame({
        'line_idx': range(len(lines)),
        'content': lines
    })
    
    # Remove empty lines
    df = df[df['content'].str.strip() != '']
    df = df.reset_index(drop=True)
    
    if len(df) == 0:
        print("Warning: No content found after removing empty lines")
        # Create minimal structure to prevent errors
        return pd.DataFrame({
            'line_idx': [0],
            'content': ["NO CONTENT EXTRACTED"],
            'section': ["DOCUMENT"],
            'section_level': [0],
            'is_section_header': [True],
            'id': [str(uuid.uuid4())]
        })
    
    # Identify sections
    current_section = "INTRODUCTION"
    sections = []
    section_levels = []
    
    for idx, line in df.iterrows():
        is_section_header = False
        level = None
        
        for pattern in section_patterns:
            if re.match(pattern, line['content'].strip()):
                current_section = line['content'].strip()
                is_section_header = True
                
                # Determine section level
                if line['content'].startswith('#'):
                    level = len(re.match(r'^(#+)', line['content']).group(1))
                elif re.match(r'^\d+\.\d+\.\d+', line['content']):
                    level = 3
                elif re.match(r'^\d+\.\d+', line['content']):
                    level = 2
                elif re.match(r'^\d+\.', line['content']):
                    level = 1
                else:
                    level = 1
                break
        
        sections.append(current_section)
        section_levels.append(level)
    
    df['section'] = sections
    df['section_level'] = section_levels
    df['is_section_header'] = [level is not None for level in section_levels]
    
    # Generate unique IDs for each entry
    df['id'] = [str(uuid.uuid4()) for _ in range(len(df))]
    
    return df

# Cell 4: Document Processing Pipeline
def process_document(pdf_path, output_dir):
    """Process PDF: extract with Tesseract, detect language, translate, and save."""
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    
    # Extract text using Tesseract
    print("Extracting text with Tesseract OCR...")
    original_text = extract_text_with_tesseract(pdf_path)
    
    if not original_text.strip():
        print("Warning: No text extracted from the document")
        return None
    
    # Get filename without extension
    base_filename = os.path.splitext(os.path.basename(pdf_path))[0]
    
    # Detect language
    print("Detecting language...")
    detected_lang = detect_language(original_text)
    print(f"Detected language: {detected_lang}")
    
    # Save original text
    original_path = os.path.join(output_dir, f"{base_filename}_original_{detected_lang}.txt")
    with open(original_path, 'w', encoding='utf-8') as f:
        f.write(original_text)
    print(f"Original text saved to: {original_path}")
    
    # Translate if not already English
    if detected_lang != 'en':
        print(f"Translating from {detected_lang} to English...")
        translated_text = translate_text(original_text, source_lang=detected_lang, target_lang='en')
        translated_path = os.path.join(output_dir, f"{base_filename}_translated_en.txt")
        with open(translated_path, 'w', encoding='utf-8') as f:
            f.write(translated_text)
        print(f"Translated text saved to: {translated_path}")
    else:
        print("Document already in English, skipping translation")
        translated_text = original_text
        translated_path = original_path
    
    return {
        'original_path': original_path,
        'translated_path': translated_path,
        'language': detected_lang,
        'original_text': original_text,
        'translated_text': translated_text
    }

def process_multilingual_document(pdf_path, output_dir, section_patterns=None):
    """
    Complete pipeline to:
    1. Extract text from PDF using Tesseract OCR
    2. Detect language
    3. Translate if needed
    4. Index and identify sections
    5. Save processed data
    """
    print(f"Processing document: {pdf_path}")
    
    # Process the document
    doc_info = process_document(pdf_path, output_dir)
    
    if doc_info is None:
        print("Document processing failed - no text extracted")
        return None, None
    
    print("Creating indexed sections for original text...")
    # Create indexed sections for original text
    original_df = create_indexed_sections(
        doc_info['original_text'],
        section_patterns=section_patterns
    )
    original_df['language'] = doc_info['language']
    original_df['is_translated'] = False
    
    # Save indexed original text
    original_excel_path = os.path.join(output_dir, f"{os.path.basename(pdf_path)}_original_indexed.xlsx")
    original_df.to_excel(original_excel_path, index=False)
    print(f"Original indexed text saved to: {original_excel_path}")
    
    # Create indexed sections for translated text (if different from original)
    if doc_info['language'] != 'en':
        print("Creating indexed sections for translated text...")
        translated_df = create_indexed_sections(
            doc_info['translated_text'],
            section_patterns=section_patterns
        )
        translated_df['language'] = 'en'
        translated_df['is_translated'] = True
        
        # Save translated dataframe
        translated_excel_path = os.path.join(output_dir, f"{os.path.basename(pdf_path)}_translated_indexed.xlsx")
        translated_df.to_excel(translated_excel_path, index=False)
        print(f"Translated indexed text saved to: {translated_excel_path}")
        
        return original_df, translated_df
    else:
        print("Document already in English, no separate translated index needed")
        return original_df, None

# Cell 5: Azure OpenAI Setup
def setup_azure_llm():
    """Set up Azure OpenAI LLM client"""
    return AzureOpenAI(
        model="gpt-4o-2024-05-13",
        api_key="redacted",
        azure_endpoint="https://cortex.aws.lmig.com/rest/v2/azure",
        api_version="2023-09-01-preview",
        default_headers={"use-case": "ERES Lease Test"}
    )

def setup_azure_embeddings():
    """Set up Azure OpenAI Embeddings"""
    return AzureOpenAIEmbeddings(
        model="text-embedding-ada-002",
        deployment="text-embedding-ada-002-v2",
        azure_endpoint="https://cortex.aws.lmig.com/rest/v2/azure",
        openai_api_key=os.environ.get("CORTEX_API_TOKEN"),
        openai_api_type="azure",
        openai_api_version="2023-09-01-preview",
        default_headers={"use-case": "ERES Lease Test"},
        chunk_size=1,
    )

# Cell 6: Embedding Functions
def embed_text_list(text_list, embeddings):
    """
    Embed a list of texts with rate limiting
    
    Args:
        text_list: List of texts to embed
        embeddings: Initialized embeddings object
        
    Returns:
        List of embedded texts
    """
    embedded_texts = []
    for text in text_list:
        embedded_text = embeddings.embed_documents([text])[0]
        embedded_texts.append(embedded_text)
        time.sleep(2)  # Adding delay to avoid rate limit
    return embedded_texts

def get_similar_texts(query, df, embeddings, embedding_column='embedding', top_n=5):
    """
    Find texts similar to a query using embeddings
    
    Args:
        query: Query text
        df: DataFrame containing texts and embeddings
        embeddings: Initialized embeddings object
        embedding_column: Column name containing embeddings
        top_n: Number of similar texts to return
        
    Returns:
        DataFrame with similar texts
    """
    # Create embedding for the query
    query_embedding = embeddings.embed_documents([query])[0]
    
    # Calculate cosine similarities
    similarities = cosine_similarity(
        np.array(query_embedding).reshape(1, -1),
        np.array(df[embedding_column].tolist())
    )
    
    # Find the indices of the top_n most similar texts
    top_indices = np.argsort(similarities.flatten())[-top_n:][::-1]
    
    # Retrieve the corresponding texts
    return df.iloc[top_indices]

# Cell 7: Interactive Chat System
class InteractiveChatQASystem:
    def __init__(
        self, 
        azure_llm,
        embeddings: AzureOpenAIEmbeddings,
        df: pd.DataFrame,
        text_column: str = 'content',
        embedding_column: str = 'embedding',
        metadata_fields: List[str] = None,
        max_history: int = 5
    ):
        self.azure_llm = azure_llm
        self.embeddings = embeddings
        self.text_column = text_column
        self.embedding_column = embedding_column
        self.max_history = max_history
        
        # Prepare metadata fields
        self.metadata_fields = metadata_fields or [
            col for col in df.columns 
            if col not in [text_column, embedding_column]
        ]
        
        # Clean and prepare data
        self.df = df.dropna(subset=[text_column, embedding_column])
        self.embeddings_array = np.array(self.df[embedding_column].tolist())
        
        # Chat history management
        self.reset_conversation()

    def reset_conversation(self):
        """Reset the conversation history."""
        self.chat_history: List[Dict[str, str]] = []
        self.conversation_context = ""
        self.last_response = None

    def get_similar_texts(self, query: str, top_n: int = 3) -> List[Dict]:
        """Retrieve the most similar texts to the query."""
        query_embedding = self.embeddings.embed_documents([query])[0]
        
        similarities = cosine_similarity(
            np.array(query_embedding).reshape(1, -1),
            self.embeddings_array
        ).flatten()
        
        top_indices = np.argsort(similarities)[-top_n:][::-1]
        
        return [
            {
                'text': self.df.iloc[idx][self.text_column],
                'metadata': {field: self.df.iloc[idx][field] for field in self.metadata_fields},
                'similarity_score': float(similarities[idx])
            }
            for idx in top_indices
        ]

    def generate_answer(self, query: str) -> Dict[str, Any]:
    """Generate a contextual answer with chat history awareness."""
    # Prepare context-aware query
    contextual_query = self._prepare_contextual_query(query)
    
    # Find similar texts
    relevant_texts = self.get_similar_texts(contextual_query)
    
    # Prepare context for LLM
    context_str = "\n\n".join([
        f"Source (similarity: {txt['similarity_score']:.3f}):\n{txt['text']}"
        for txt in relevant_texts
    ])
    
    # Construct prompt with historical context
    prompt = f"""Conversation History:
{self.conversation_context}

Current Question: "{query}"

Relevant Information:
{'-' * 40}
{context_str}
{'-' * 40}

Guidelines for response:
1. Directly address the current question
2. Consider the conversation history
3. Use information from the sources
4. Maintain a natural conversational flow
5. If the question cannot be fully answered, explain why

Answer:"""
    
    # Generate response
    try:
        # Use invoke with text parameter for Azure OpenAI
        response = self.azure_llm.invoke(prompt)
        
        # Check response format and extract text
        if hasattr(response, 'content'):
            response_text = response.content
        elif isinstance(response, str):
            response_text = response
        else:
            response_text = str(response)
            
    except Exception as e:
        print(f"An error occurred while generating response: {e}")
        response_text = "I'm sorry, I couldn't generate a response at this time."
    
    # Update chat history
    self._update_chat_history(query, response_text, relevant_texts)
    
    return {
        'answer': response_text,
        'sources': relevant_texts
    }

    def _prepare_contextual_query(self, query: str) -> str:
        """Prepare a context-aware query by incorporating recent conversation history."""
        if self.chat_history:
            # Create a summary of recent interactions
            recent_context = " ".join([
                f"{entry['role']}: {entry['text']}"
                for entry in self.chat_history[-self.max_history:]
            ])
            return f"{recent_context}\n\nLatest Query: {query}"
        return query

    def _update_chat_history(self, query: str, response: str, sources: List[Dict]):
        """Update chat history and conversation context."""
        # Add current interaction to history
        self.chat_history.append({
            'role': 'user', 
            'text': query
        })
        self.chat_history.append({
            'role': 'assistant', 
            'text': response
        })
        
        # Maintain a reasonable history length
        self.chat_history = self.chat_history[-self.max_history:]
        
        # Update conversation context
        self.conversation_context = "\n".join([
            f"{entry['role'].upper()}: {entry['text']}"
            for entry in self.chat_history
        ])
        
        # Store the last response for reference
        self.last_response = response

    def interactive_chat(self):
        """
        Interactive chat interface in the notebook.
        Allows continuous questioning until the user chooses to exit.
        """
        print("Welcome to the Interactive Q&A System!")
        print("Type your questions, or 'exit' to end the conversation.")
        print("Type 'reset' to start a new conversation.")
        
        while True:
            # Get user input
            user_input = input("\nYou: ").strip()
            
            # Check for exit conditions
            if user_input.lower() == 'exit':
                print("Goodbye!")
                break
            
            # Check for reset
            if user_input.lower() == 'reset':
                self.reset_conversation()
                print("Conversation reset. Ready for a new discussion.")
                continue
            
            # Generate and display answer
            try:
                result = self.generate_answer(user_input)
                print("\nAssistant:", result['answer'])
                
                # Optionally display sources
                print("\nSources:")
                for source in result['sources']:
                    print(f"- Similarity: {source['similarity_score']:.3f}")
                    print(f"  Content: {source['text'][:200]}...")
            
            except Exception as e:
                print(f"An error occurred: {e}")

# Cell 8: Main Processing Function with Embedding
def process_and_create_qa_system(pdf_path, output_dir):
    """
    Process a document and create an interactive QA system
    
    Args:
        pdf_path: Path to the PDF document
        output_dir: Directory to save processed files
        
    Returns:
        Interactive QA system
    """
    # Process the document
    original_df, translated_df = process_multilingual_document(pdf_path, output_dir)
    
    # Use the translated dataframe if available, otherwise use the original
    master_df = translated_df if translated_df is not None else original_df
    
    # Initialize Azure OpenAI clients
    azure_llm = setup_azure_llm()
    azure_embeddings = setup_azure_embeddings()
    
    print("Creating embeddings for the document content...")
    # Create embeddings for the text
    master_df['embedding'] = embed_text_list(master_df['content'].tolist(), azure_embeddings)
    
    # Save the dataframe with embeddings
    embedding_path = os.path.join(output_dir, f"{os.path.basename(pdf_path)}_with_embeddings.pkl")
    master_df.to_pickle(embedding_path)
    print(f"DataFrame with embeddings saved to: {embedding_path}")
    
    # Create the interactive QA system
    interactive_system = InteractiveChatQASystem(
        azure_llm=azure_llm,
        embeddings=azure_embeddings,
        df=master_df
    )
    
    return interactive_system

# Cell 9: Load from Saved Embeddings
def load_qa_system_from_saved(embedding_path):
    """
    Load a QA system from a saved DataFrame with embeddings
    
    Args:
        embedding_path: Path to the saved DataFrame with embeddings
        
    Returns:
        Interactive QA system
    """
    # Load the dataframe with embeddings
    master_df = pd.read_pickle(embedding_path)
    
    # Initialize Azure OpenAI clients
    azure_llm = setup_azure_llm()
    azure_embeddings = setup_azure_embeddings()
    
    # Create the interactive QA system
    interactive_system = InteractiveChatQASystem(
        azure_llm=azure_llm,
        embeddings=azure_embeddings,
        df=master_df
    )
    
    return interactive_system

# Cell 10: Example Usage
# Example usage - uncomment and customize paths to run
"""
# Process a new document and create a QA system
pdf_path = 'path/to/your/document.pdf'
output_dir = 'path/to/output/directory'

# Option 1: Process document from scratch
qa_system = process_and_create_qa_system(pdf_path, output_dir)
qa_system.interactive_chat()

# Option 2: Load from previously processed document
# embedding_path = 'path/to/saved/document_with_embeddings.pkl'
# qa_system = load_qa_system_from_saved(embedding_path)
# qa_system.interactive_chat()
"""
